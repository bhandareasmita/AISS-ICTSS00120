## AI Skillset Course Glossary

See also: [Google AI Glossary](https://developers.google.com/machine-learning/glossary)

### A
- **Accuracy**: The ratio of correctly predicted instances to the total instances in a dataset. \(\text{Accuracy} = \frac{\text{True Positives + True Negatives}}{\text{True Positives + True Negatives + False Positives + False Negatives}}\).
- **Activation Function**: A function applied to each neuron in a neural network to introduce non-linearity, enabling the network to learn complex patterns. Examples include ReLU, Sigmoid, and Tanh.
- **Artificial Intelligence (AI)**: The simulation of human intelligence in machines programmed to think and act like humans. AI technologies perform tasks such as decision-making, speech recognition, and language translation.

### B
- **Backpropagation**: A training algorithm for neural networks that involves propagating the error backward through the network to update weights using the chain rule.

### C
- **Classification**: A type of supervised learning where the goal is to predict a discrete label or category for a given input. Examples include spam detection and image recognition.
- **Clustering**: An unsupervised learning technique that groups data into clusters based on similarity. Examples include K-Means and DBSCAN.
- **Convolutional Neural Network (CNN)**: A specialized neural network designed for processing structured grid data like images. Key components include convolutional layers and pooling layers.

### D
- **Data Augmentation**: Techniques applied to training data to increase its size and diversity, improving model generalization. Methods include rotations and flips for images.
- **Deep Learning (DL)**: A subset of machine learning involving neural networks with multiple layers (deep neural networks) that can learn from large amounts of data. Used in applications such as image recognition and natural language processing.
- **Dimensionality Reduction**: Techniques used to reduce the number of input variables in a dataset, simplifying models and preventing overfitting. Examples include PCA (Principal Component Analysis).

### E
- **Epoch**: One complete pass through the entire training dataset during the training process.
- **Exploratory Data Analysis (EDA)**: Analyzing datasets to summarize their main characteristics, often with visual methods. Helps in understanding data structure, outliers, and distribution.
- **Evaluation Metrics**: Measures used to evaluate the performance of a machine learning model. Common metrics include accuracy, recall, precision, F1 score, and confusion matrix.

### F
- **Feed Forward Neural Network (FNN)**: The simplest type of artificial neural network where connections between nodes do not form cycles. Used for tasks such as image and speech recognition.
- **F1 Score**: The harmonic mean of precision and recall, balancing both metrics. Useful in cases with imbalanced class distributions.
- **False Positive (FP)**: An incorrect prediction where the model wrongly predicts a positive class.

### G
- **Gradient Descent**: An optimization algorithm used to minimize the cost function by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient.
- **Grid Search**: A method for hyperparameter tuning that exhaustively searches through a specified subset of hyperparameter space to find the best model performance.

### H
- **Hidden Layer**: Intermediate layers between the input and output layers in a neural network where computations are performed to extract complex features and patterns.
- **Hyperparameter**: Parameters set before the learning process begins and control the training of the model, such as learning rate and number of layers.
- **Hyperparameter Tuning**: The process of optimizing hyperparameters to improve model performance. Techniques include grid search and random search.

### I
- **Input Layer**: The first layer in a neural network that receives the input data. Each neuron in this layer represents a feature of the input.
- **Instance**: A single observation or data point used for training or testing a machine learning model.

### J
- **Jupyter Notebook**: An open-source web application that allows for interactive computing with live code, equations, visualizations, and narrative text. Widely used in data science and machine learning.

### K
- **K-Means Clustering**: An unsupervised learning algorithm that partitions the data into K clusters based on feature similarity by minimizing the within-cluster sum of squares.

### L
- **Label**: The output or target variable that a machine learning model predicts. Used in supervised learning for model training.
- **Learning Rate**: A hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated.
- **Linear Regression**: A supervised learning algorithm used to predict continuous values by modeling the relationship between a dependent variable and one or more independent variables.

### M
- **Machine Learning (ML)**: A subset of AI that involves the use of algorithms and statistical models to enable computers to learn from and make predictions or decisions based on data.
- **Model**: A computational representation of the relationship between input data and output predictions. Can be trained and tested on data to make predictions.

### N
- **Neural Network (NN)**: A series of algorithms that attempt to recognize underlying relationships in data through a process mimicking the human brain. Composed of layers of nodes (neurons).
- **Normalization**: A technique to adjust values measured on different scales to a common scale, enhancing model performance and training stability.

### O
- **Overfitting**: A modeling error that occurs when a model is too closely fitted to the training data, leading to poor generalization on unseen data. Prevented by techniques like regularization and cross-validation.
- **Optimizer**: An algorithm used to adjust the parameters of a neural network to minimize the loss function. Popular optimizers include SGD (Stochastic Gradient Descent), Adam, and RMSprop.

### P
- **Precision**: The ratio of true positive predictions to the sum of true positive and false-positive predictions. Measures the accuracy of positive predictions. \(\text{Precision} = \frac{\text{True Positives}}{\text{True Positives + False Positives}}\).
- **Principal Component Analysis (PCA)**: A dimensionality reduction technique that transforms data into a new coordinate system by emphasizing variance.

### R
- **Recall (Sensitivity)**: The ratio of true positive predictions to the sum of true positive and false negative predictions. Measures how well the model identifies positive instances.
- **ReLU (Rectified Linear Unit)**: An activation function defined as the positive part of its argument, commonly used in neural networks to introduce non-linearity.
- **Reinforcement Learning (RL)**: A type of machine learning where an agent interacts with an environment to learn actions that maximize cumulative rewards.

### S
- **Supervised Learning**: A type of machine learning where the model is trained on labeled data, learning to map inputs to outputs.
- **Support Vector Machine (SVM)**: A supervised machine learning algorithm used for classification and regression tasks that finds the hyperplane that best separates classes.
- **Stochastic Gradient Descent (SGD)**: An optimization algorithm that updates model parameters using one training example per iteration. It is computationally efficient and effective for large datasets.

### T
- **TensorFlow**: An open-source framework for machine learning and deep learning developed by Google. Used to develop and train neural network models.
- **Training Set**: A set of data used to train a machine learning model, enabling the model to learn patterns and relationships.

### U
- **Unsupervised Learning**: A type of machine learning where the model learns patterns from unlabeled data, such as clustering and dimensionality reduction.

### V
- **Validation Set**: A subset of the data not used for training but for tuning hyperparameters and model selection during training. Helps prevent overfitting.

### W
- **Weight**: Parameters within a neural network that are adjusted during training to minimize the error of the model. Represent the strength of connections between neurons.
